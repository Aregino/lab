# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {}
# META }

# MARKDOWN ********************

# # Trabalhar com o Estruturador de Dados


# MARKDOWN ********************

# O Estruturador de Dados pode ajudar na fase de pré-processamento da criação de um modelo de machine learning, fornecendo ferramentas e funcionalidades para limpeza de dados, definição de recursos, exploração de dados e melhoria da eficiência no pré-processamento de dados.
# 
# **Exploração de dados:** A exibição de dados como grade da ferramenta permite explorar visualmente seus dados, o que pode levar a insights sobre variáveis.
# 
# **Limpeza de dados:** O Estruturador de Dados fornece uma biblioteca de operações comuns de limpeza de dados, facilitando o tratamento de valores ausentes, exceções e tipos de dados incorretos.
# 
# **Definição de recursos:** Com suas visualizações integradas e estatísticas de resumo dinâmico, o Estruturador de Dados pode ajudá-lo a entender a distribuição de seus dados e criar recursos.
# 
# O Estruturador de Dados pode ajudar a garantir que seus dados estejam na melhor forma possível antes de serem usados para treinar um modelo de machine learning. Isso pode levar a modelos mais precisos e melhores previsões.


# MARKDOWN ********************

# **Iniciar o Estruturador de Dados a partir de um notebook**

# MARKDOWN ********************

# - O Estruturador de Dados é uma ferramenta compilada em notebooks do Microsoft Fabric que oferece uma plataforma abrangente para tarefas exploratórias e de pré-processamento. Ele oferece uma exibição de dados, estatísticas de resumo dinâmico, visualizações integradas e uma biblioteca de operações comuns de pré-processamento de dados.

# CELL ********************

# Seria uma analise usando o Data Wrangler

import pandas as pd
df = pd.read_csv("https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ### **Executar a exploração de dados**
# 
# O Data Wrangler facilita a exploração dos seus dados com uma interface de grade fácil de usar que apresenta dinamicamente estatísticas resumidas dos seus dados.
# 
# Por meio da exploração visual de estatísticas resumidas, os cientistas de dados podem selecionar os modelos de machine learning ou estatísticos apropriados que melhor se ajustam aos dados. Por exemplo, alguns modelos presumem que os dados são normalmente distribuídos e podem não ter um bom desempenho se essa suposição for violada.

# MARKDOWN ********************

# ### **Exibir estatísticas resumidas**

# CELL ********************

# Para fins de demonstração, vamos gerar alguns dados aleatórios para simular um cenário hipotético envolvendo preços de casas em um determinado bairro.
import pandas as pd
import numpy as np

# Set the seed
np.random.seed(0)

# Define the size of the dataset
size = 500

# Generate random data
data = {
    'Size': np.random.randint(1000, 4001, size, dtype=int) // 10 * 10, # any integer value between 1000 and 4000, with multiple of 10
    'Bedrooms': np.random.choice([2, 4, 3, 2, 1], size),
    'YearBuilt': np.random.randint(1980, 2021, size), # any integer value between 1980 and 2020
    'Price': np.random.normal(loc=110000, scale=20000, size=size), # normally distributed prices
    'Type': np.random.choice(['Single Family', 'Townhouse', 'Condo', 'Duplex'], size) # type of the house
}

# Create a DataFrame
df = pd.DataFrame(data)

# iniciar o data Wrangler

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ### **Tratar dados ausentes**
# 
# Dados ausentes referem-se à falta de valores em determinadas variáveis dentro de um conjunto de dados.
# 
# Lidar com dados ausentes é um aspecto crucial da fase de pré-processamento em um projeto de machine learning e a maneira como você os trata pode afetar significativamente o desempenho do seu modelo

# CELL ********************

# Code generated by Data Wrangler for pandas DataFrame

def clean_data(df):
    # Drop rows with missing data in column: 'Price'
    df = df.dropna(subset=['Price'])
    # Replace missing values with the median of each column in: 'YearBuilt'
    df = df.fillna({'YearBuilt': df['YearBuilt'].median()})
    return df

df_clean = clean_data(df.copy())
df_clean.head()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ### **Transformar dados com operadores**
# 
# Enquanto muitos operadores no Estruturador de Dados são intuitivos e fáceis de usar, outros exigem uma compreensão mais profunda para usá-los completamente.
# 
# - #### Usar um operador de codificação one-hot
# 
# Alguns modelos de machine learning, como regressão linear, exigem que todas as variáveis de entrada e saída sejam numéricas e não ofereçam suporte a variáveis categóricas. Dados categóricos se referem a variáveis que são divididas em várias categorias que não carregam uma ordem ou valor numérico.
# 
# Em uma codificação one-hot, cada categoria em um recurso é representada como um vetor binário de 1 e 0.
# 
# Por exemplo, se você tiver uma variável animal de estimação com os valores cão, gato e pássaro, três novas variáveis serão criadas (uma para cada tipo de animal de estimação). Para cada ponto de dados, ele marca 1 para o animal de estimação que representa e 0 para os outros. Assim, se um ponto de dados representa um cão, ele é codificado como [1, 0, 0]. Se for um gato, é [0, 1, 0], e se for um pássaro, é [0, 0, 1].


# CELL ********************


# Sample dataset with 50 data points, including duplicates
data = {'pet': ['dog', 'dog', 'cat', 'cat', 'bird', 'bird']*8 + ['bird', 'cat']}
df = pd.DataFrame(data)


'''
As etapas a seguir mostram como usar o operador da codificação one-hot para a variável pet.

Iniciar o Estruturador de Dados em um notebook do Microsoft Fabric para o dataframe df.

Selecione a variável pet.

No painel Operações, selecione Fórmulas e Codificação one-hot.

'''

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for pandas DataFrame

import pandas as pd

def clean_data(df):
    # One-hot encode column: 'pet'
    insert_loc = df.columns.get_loc('pet')
    df = pd.concat([df.iloc[:,:insert_loc], pd.get_dummies(df.loc[:, ['pet']]), df.iloc[:,insert_loc+1:]], axis=1)
    return df

df_clean = clean_data(df.copy())
df_clean.head()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ### **Usar o operador binarizador de vários rótulos**
# 
# A codificação one-hot é usada quando cada ponto de dados pertence exatamente a uma categoria. Por outro lado, o operador binarizador de vários rótulos é usado quando cada ponto de dados pode pertencer a várias categorias.
# 
# O operador binarizador de vários rótulos permite dividir dados categóricos em uma nova coluna para cada categoria usando um delimitador de divisão de texto, em que cada nova coluna contém um número um em linhas que correspondem a essa categoria e, caso contrário, 0.
# 
# Para fins de treinamento, vamos criar um dataframe sobre a categoria de alimentos e usar o Estruturador de Dados para gerar o código para o binarizador de vários rótulos.

# CELL ********************

import pandas as pd

#Sample data
data = {
    'food': ['Pasta', 'Burger', 'Ice Cream', 'Salad'],
    'category': ['Italian|Fine dining', 'American|Fast Food', 'Dessert', 'Healthy']
}

# Create DataFrame
restaurant = pd.DataFrame(data)
display(restaurant)

'''
Em seguida, as etapas a seguir mostram como usar o operador binarizador de vários rótulos para a variável category.

Iniciar o Estruturador de Dados em um notebook do Microsoft Fabric para o dataframe restaurant.

Selecione a variável category.

No painel Operações, selecione Fórmulas e, em seguida, Binarizador de vários rótulos.

Digite | como o delimitador.


O resultado é um dataframe com variáveis para cada categoria, como Americana, Sobremesa, Fast Food, Saudável e Italiana. 
Cada item alimentar é marcado com 1 ou 0 nessas colunas para mostrar 
a que categorias pertence. Por exemplo, tanto a Pizza quanto o Hambúrguer se enquadram em várias categorias.
'''

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for pandas DataFrame

import pandas as pd

def clean_data(restaurant):
    # Performed multi-label encoding on column 'category' split with delimiter '|'
    loc_0 = restaurant.columns.get_loc('category')
    restaurant_encoded = restaurant['category'].str.get_dummies(sep='|').add_prefix('category_')
    restaurant = pd.concat([restaurant.iloc[:,:loc_0], restaurant_encoded, restaurant.iloc[:,loc_0+1:]], axis=1)
    return restaurant

restaurant_clean = clean_data(restaurant.copy())
restaurant_clean.head()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ###  **Usar operador de escala mín.–máx.**
# 
# 
# [Escala mín.–máx. ou normalização mín.-máx.](https://learn.microsoft.com/pt-br/dotnet/machine-learning/how-to-guides/prepare-data-ml-net#min-max-normalization?azure-portal=true) é o processo de transformação de um recurso numérico. Esse processo escala o intervalo dos seus dados, preservando a forma da distribuição original e as relações entre as variáveis.
# 
# Ele garante que a significância de um recurso seja determinada por seu valor relativo, não pelo seu valor absoluto. Em outras palavras, os recursos não são considerados mais importantes simplesmente porque têm escalas maiores.
# 
# Ele pega cada valor em seus dados, subtrai o valor mínimo desses dados e, em seguida, divide pelo intervalo dos dados (o valor máximo menos o valor mínimo).
# 
# O resultado é que seus dados são redimensionados para um intervalo de 0 a 1 normalmente, o que pode ser útil para certos tipos de algoritmos de aprendizado de máquina, particularmente aqueles que usam medidas de distância como [K-Vizinho mais próximo.](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
# 
# Vamos considerar um dataframe que representa as notas de alunos em uma classe. O dataframe tem três colunas:
# 
#     Student, Math_Grade, English_Grade e Hours_Studied


# CELL ********************

import pandas as pd

# Sample data
data = {
    'Student': ['Bob', 'Mark', 'Anna', 'David', 'Sam'],
    'Math_Grade': [85, 90, 78, 92, 88],
    'English_Grade': [80, 85, 92, 88, 90],
    'Hours_Studied': [250, 500, 355, 245, 199] 
}

df = pd.DataFrame(data)

print(df)

'''
Agora, vamos aplicar o dimensionador mín.-máx. às variáveis, Math_Grade, English_Grade e Hours_Studied usando o 
Estruturador de Dados. Para isso, você precisa usar o operador Escala de valores mín./máx. na categoria Numérica.
'''


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Code generated by Data Wrangler for pandas DataFrame

def clean_data(df):
    # Scale columns 'Math_Grade', 'English_Grade', 'Hours_Studied' between 0 and 1
    new_min, new_max = 0, 1
    old_min, old_max = df['Math_Grade'].min(), df['Math_Grade'].max()
    df['Math_Grade'] = (df['Math_Grade'] - old_min) / (old_max - old_min) * (new_max - new_min) + new_min
    old_min, old_max = df['English_Grade'].min(), df['English_Grade'].max()
    df['English_Grade'] = (df['English_Grade'] - old_min) / (old_max - old_min) * (new_max - new_min) + new_min
    old_min, old_max = df['Hours_Studied'].min(), df['Hours_Studied'].max()
    df['Hours_Studied'] = (df['Hours_Studied'] - old_min) / (old_max - old_min) * (new_max - new_min) + new_min
    return df

df_clean = clean_data(df.copy())
df_clean.head()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# Acima, as notas são dimensionadas para se enquadrarem no intervalo de [0, 1], com a nota mínima mapeada para 0 e a nota máxima mapeada para 1. Outras notas são dimensionadas proporcionalmente dentro desse intervalo. Você também pode ajustar o intervalo mínimo e máximo.
# 
# Se você usar recursos como **Math_Grade, English_Grade e Hours_Studied** em um algoritmo de aprendizado de máquina baseado em distância, como [K-Vizinho mais próximo](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), sem primeiro dimensioná-los, poderá encontrar alguns problemas.
# 
# O recurso Hours_Studied poderia potencialmente dominar os outros recursos devido ao seu maior intervalo de valores. Isso poderia levar a um modelo que depende fortemente de Hours_Studied, ignorando Math_Grade e English_Grade. Portanto, é importante dimensionar seus dados nesses casos para garantir que todos os recursos recebam a mesma importância.
# 
# Para saber mais sobre normalização de dados para modelos de machine learning, consulte [Tranformações de dados](https://learn.microsoft.com/pt-br/dotnet/machine-learning/resources/transforms#normalization-and-scaling?azure-portal=true)

